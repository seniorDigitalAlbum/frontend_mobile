import { useState, useEffect, useCallback } from 'react';
import { useNavigation } from '@react-navigation/native';
import { useUser } from '../contexts/UserContext';
import sttService from '../services/audio/sttService';
import { ConversationService, EmotionCapture } from '../services/conversationService';
import { ConversationUtils } from '../utils/conversationUtils';

export interface UseConversationParams {
    questionText?: string;
    questionId?: string;
    conversationId?: number;
    cameraSessionId?: string;
    microphoneSessionId?: string;
}

export interface UseConversationReturn {
    // ìƒíƒœ
    isRecording: boolean;
    transcribedText: string | null;
    isQuestionComplete: boolean;
    currentQuestionText: string;
    isProcessingResponse: boolean;
    hasAIResponse: boolean;
    emotionCaptures: EmotionCapture[];
    isQuestionTTSPlayed: boolean;
    userId: string;
    questionId: string;
    conversationId: number | undefined;
    cameraSessionId: string | undefined;
    microphoneSessionId: string | undefined;
    
    // í•¨ìˆ˜
    handleQuestionComplete: () => void;
    handleEndChat: () => Promise<void>;
    handleAnswerRecordingComplete: (audioUri: string, questionId: string) => Promise<void>;
    handleAnswerRecordingStart: (questionId: string) => Promise<void>;
    handleAIResponse: (userText: string, audioBase64?: string, conversationMessageId?: number) => Promise<void>;
    setCurrentQuestionText: (text: string) => void;
    setTranscribedText: (text: string | null) => void;
    setEmotionCaptures: (captures: EmotionCapture[]) => void;
    addEmotionCapture: (capture: EmotionCapture) => void;
}

export const useConversation = (params: UseConversationParams): UseConversationReturn => {
    const navigation = useNavigation();
    const { user } = useUser();
    
    // íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    const safeQuestionText = ConversationUtils.getSafeQuestionText(params.questionText);
    const userId = ConversationUtils.getUserId(user?.userId);
    
    // ìƒíƒœ
    const [isRecording, setIsRecording] = useState(false);
    const [transcribedText, setTranscribedText] = useState<string | null>(null);
    const [isQuestionComplete, setIsQuestionComplete] = useState(false);
    const [currentQuestionText, setCurrentQuestionText] = useState(safeQuestionText);
    const [isProcessingResponse, setIsProcessingResponse] = useState(false);
    const [hasAIResponse, setHasAIResponse] = useState(false);
    const [emotionCaptures, setEmotionCaptures] = useState<EmotionCapture[]>([]);
    const [isQuestionTTSPlayed, setIsQuestionTTSPlayed] = useState(false);

    // AI ì§ˆë¬¸ ë©”ì‹œì§€ ì €ì¥
    useEffect(() => {
        const initializeQuestion = async () => {
            if (params.conversationId && safeQuestionText && !isQuestionTTSPlayed) {
                try {
                    await ConversationService.saveAIMessage(params.conversationId, safeQuestionText);
                    setIsQuestionTTSPlayed(true);
                } catch (error) {
                    console.error('ì§ˆë¬¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
                }
            }
        };
        
        initializeQuestion();
    }, [params.conversationId, safeQuestionText, isQuestionTTSPlayed]);

    // STT ì„œë¹„ìŠ¤ ì •ë¦¬
    useEffect(() => {
        return () => {
            sttService.cleanup();
        };
    }, []);

    // ì§ˆë¬¸ ì™„ë£Œ í•¸ë“¤ëŸ¬
    const handleQuestionComplete = useCallback(() => {
        console.log('ğŸµ TTS ì¬ìƒ ì™„ë£Œ - ë§ˆì´í¬ ë²„íŠ¼ í‘œì‹œ');
        setIsQuestionComplete(true);
    }, []);


    // ëŒ€í™” ì¢…ë£Œ í•¸ë“¤ëŸ¬
    const handleEndChat = useCallback(async () => {
        try {
            if (params.conversationId) {
                const result = await ConversationService.endConversation(params.conversationId);
                
                if (result.success && ConversationUtils.shouldEndConversation(result.data)) {
                    navigation.navigate('ConversationEndLoading', {
                        conversationId: params.conversationId
                    });
                }
            }
        } catch (error) {
            console.error('ëŒ€í™” ì¢…ë£Œ ì‹¤íŒ¨:', error);
            ConversationUtils.showErrorAlert('ëŒ€í™”ë¥¼ ì¢…ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
        }
    }, [params.conversationId, navigation]);

    // ë‹µë³€ ë…¹ìŒ ì™„ë£Œ í•¸ë“¤ëŸ¬
    const handleAnswerRecordingComplete = useCallback(async (audioUri: string, questionId: string) => {
        console.log(`ë‹µë³€ ë…¹ìŒ ì™„ë£Œ - ì§ˆë¬¸ ID: ${questionId}, ì˜¤ë””ì˜¤ URI: ${audioUri}`);
        setIsRecording(false);
    }, []);

    // ë‹µë³€ ë…¹ìŒ ì‹œì‘ í•¸ë“¤ëŸ¬
    const handleAnswerRecordingStart = useCallback(async (questionId: string) => {
        console.log(`ë‹µë³€ ë…¹ìŒ ì‹œì‘ - ì§ˆë¬¸ ID: ${questionId}`);
        setIsRecording(true);
    }, []);

    // AI ì‘ë‹µ í•¸ë“¤ëŸ¬
    const handleAIResponse = useCallback(async (userText: string, audioBase64?: string, conversationMessageId?: number) => {
        console.log('ì‚¬ìš©ì ë°œí™” í…ìŠ¤íŠ¸ ë°›ìŒ:', userText);
        console.log('conversationMessageId ë°›ìŒ:', conversationMessageId);
        
        // ë¹ˆ ê°’ì´ë©´ ì¦‰ì‹œ ì²˜ë¦¬ ìƒíƒœ ì‹œì‘ (ë§ˆì´í¬ ë²„íŠ¼ì„ ë‹¤ì‹œ ëˆŒë €ì„ ë•Œ)
        if (!userText || userText.trim() === '') {
            console.log('ì¦‰ì‹œ ì²˜ë¦¬ ìƒíƒœ ì‹œì‘ - ë‹¤ìŒë§ì„ ìƒì„±í•˜ê³  ìˆì–´ìš”...');
            setIsProcessingResponse(true);
            setIsQuestionComplete(false);
            setHasAIResponse(false); // AI ì‘ë‹µ ìƒíƒœ ë¦¬ì…‹
            return;
        }
        
        // STT ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬
        if (!ConversationUtils.isValidSTTResult(userText)) {
            console.log('STT ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ - TTSë¡œ ì¬ì‹œë„ ë©”ì‹œì§€ ì¬ìƒ');
            
            const result = await ConversationService.playSTTErrorMessage();
            if (result.success) {
                setIsQuestionComplete(true);
            }
            return;
        }
        
        try {
            // 1. ì‚¬ìš©ì ë‹µë³€ ì²˜ë¦¬ ì‹œì‘ - "ë‹¤ìŒë§ì„ ìƒì„±í•˜ê³  ìˆì–´ìš”..." í‘œì‹œ
            setIsProcessingResponse(true);
            setTranscribedText(userText);
            setIsQuestionComplete(false); // ë§ˆì´í¬ ë²„íŠ¼ ìˆ¨ê¹€
            
            // ê°ì • ë¶„ì„ ê²°ê³¼ ì „ì†¡
            if (emotionCaptures.length > 0 && conversationMessageId) {
                await ConversationService.sendFacialEmotionAnalysis(conversationMessageId, emotionCaptures);
                await ConversationService.executeKoBERTFlow(conversationMessageId, userText);
            } else if (conversationMessageId) {
                await ConversationService.executeKoBERTFlow(conversationMessageId, userText);
            }

            // 2. GPT ë‹µë³€ ìƒì„±
            if (conversationMessageId) {
                const gptResponse = await ConversationService.generateAIResponse(conversationMessageId);
                
                if (gptResponse.success && gptResponse.data) {
                    // 1. AI ì‘ë‹µì„ ë§í’ì„ ì— ë¨¼ì € í‘œì‹œ
                    setCurrentQuestionText(gptResponse.data.aiResponse);
                    setHasAIResponse(true);
                    
                    // 2. ì ì‹œ ëŒ€ê¸° í›„ TTS ì¬ìƒ
                    setTimeout(async () => {
                        await ConversationService.playAIResponseTTS(gptResponse.data.aiResponse);
                        
                        // 3. TTS ì¬ìƒ ì™„ë£Œ í›„ ë§ˆì´í¬ ë²„íŠ¼ ë‹¤ì‹œ í‘œì‹œ
                        setIsQuestionComplete(true);
                        setIsProcessingResponse(false);
                    }, 500); // 0.5ì´ˆ ëŒ€ê¸°
                } else {
                    console.error('AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨');
                    setIsProcessingResponse(false);
                    setIsQuestionComplete(true);
                }
            }
            
        } catch (error) {
            console.error('AI ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);
            setIsProcessingResponse(false);
            setIsQuestionComplete(true);
        }
    }, [emotionCaptures]);

    // ê°ì • ìº¡ì²˜ ì¶”ê°€
    const addEmotionCapture = useCallback((capture: EmotionCapture) => {
        setEmotionCaptures(prev => [...prev, capture]);
        console.log('ê°ì • ìº¡ì²˜ ì¶”ê°€ë¨:', capture);
    }, []);

    return {
        // ìƒíƒœ
        isRecording,
        transcribedText,
        isQuestionComplete,
        currentQuestionText,
        isProcessingResponse,
        hasAIResponse,
        emotionCaptures,
        isQuestionTTSPlayed,
        userId,
        questionId: params.questionId || 'default-question-id',
        conversationId: params.conversationId,
        cameraSessionId: params.cameraSessionId,
        microphoneSessionId: params.microphoneSessionId,
        
        // í•¨ìˆ˜
        handleQuestionComplete,
        handleEndChat,
        handleAnswerRecordingComplete,
        handleAnswerRecordingStart,
        handleAIResponse,
        setCurrentQuestionText,
        setTranscribedText,
        setEmotionCaptures,
        addEmotionCapture,
    };
};
