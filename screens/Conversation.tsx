import { View, SafeAreaView, Text, Alert } from 'react-native';
import { NativeStackScreenProps } from '@react-navigation/native-stack';
import { RootStackParamList } from '../App';
import { useState, useEffect } from 'react';
import { useAccessibility } from '../contexts/AccessibilityContext';
import HiddenCamera from '../components/HiddenCamera';
import AIQuestionSection from '../components/AIQuestionSection';
import UserAnswerSection from '../components/UserAnswerSection';
import sttService from '../services/audio/sttService';
import ttsService from '../services/audio/ttsService';
import conversationApiService from '../services/api/conversationApiService';
import microphoneApiService from '../services/api/microphoneApiService';
import ttsApiService from '../services/api/ttsApiService';
import gptApiService from '../services/api/gptApiService';
import { sendFacialEmotionAnalysis } from '../services/api/emotionApiService';
import conversationContextApiService from '../services/api/conversationContextApiService';
import kobertApiService from '../services/api/kobertApiService';
import speechEmotionApiService from '../services/api/speechEmotionApiService';
import combinedEmotionApiService from '../services/api/combinedEmotionApiService';

type Props = NativeStackScreenProps<RootStackParamList, 'Conversation'>;

export default function Conversation({ route, navigation }: Props) {
    const { settings } = useAccessibility();
    const { 
        questionText, 
        questionId, 
        conversationId, 
        cameraSessionId, 
        microphoneSessionId
    } = route.params || {};
    
    // userId í•˜ë“œì½”ë”©
    const userId = "1";
    
    // questionTextê°€ nullì´ê±°ë‚˜ undefinedì¸ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
    const safeQuestionText = questionText || 'ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”?';
    
    // console.log('Conversation.tsx - route.params:', route.params);
    // console.log('Conversation.tsx - questionText:', questionText);
    // console.log('Conversation.tsx - safeQuestionText:', safeQuestionText);
    
    const [isRecording, setIsRecording] = useState(false);
    const [transcribedText, setTranscribedText] = useState<string | null>(null);
    const [isQuestionComplete, setIsQuestionComplete] = useState(false);
    const [currentQuestionText, setCurrentQuestionText] = useState(safeQuestionText);
    const [originalQuestionText] = useState(safeQuestionText); // ì›ë³¸ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ë³´ê´€
    const [isProcessingResponse, setIsProcessingResponse] = useState(false);
    const [hasAIResponse, setHasAIResponse] = useState(false);
    const [emotionCaptures, setEmotionCaptures] = useState<Array<{
        timestamp: string;
        emotion: string;
        confidence: number;
    }>>([]);
    const [isQuestionTTSPlayed, setIsQuestionTTSPlayed] = useState(false);

    // AI ì§ˆë¬¸ ë©”ì‹œì§€ ì €ì¥ (ì„ì‹œ ë¹„í™œì„±í™” - ë°±ì—”ë“œ ì—”ë“œí¬ì¸íŠ¸ ì—†ìŒ)
    useEffect(() => {
        const initializeQuestion = async () => {
            if (conversationId && safeQuestionText && !isQuestionTTSPlayed) {
                try {
                    // AI ì§ˆë¬¸ ë©”ì‹œì§€ ì €ì¥ (ì„ì‹œ ë¹„í™œì„±í™”)
                    // await conversationApiService.saveAIMessage(conversationId, safeQuestionText);
                    console.log('AI ì§ˆë¬¸ ë©”ì‹œì§€ ì €ì¥ë¨:', safeQuestionText);
                    setIsQuestionTTSPlayed(true);
                    // TTS ì¬ìƒì´ ì™„ë£Œëœ í›„ì— ë§ˆì´í¬ ë²„íŠ¼ì´ ë³´ì´ë„ë¡ í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” isQuestionCompleteë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŒ
                } catch (error) {
                    console.error('ì§ˆë¬¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
                }
            }
        };
        
        initializeQuestion();
    }, [conversationId, safeQuestionText, isQuestionTTSPlayed]);

    // STT ì„œë¹„ìŠ¤ ì •ë¦¬
    useEffect(() => {
        return () => {
            sttService.cleanup();
        };
    }, []);

    const handleQuestionComplete = () => {
        console.log('ğŸµ TTS ì¬ìƒ ì™„ë£Œ - ë§ˆì´í¬ ë²„íŠ¼ í‘œì‹œ');
        setIsQuestionComplete(true);
    };

    const handleNext = () => {
        // AI ì‘ë‹µì„ ë°›ì•˜ì„ ë•Œë§Œ ë‹¤ìŒ í™”ë©´ìœ¼ë¡œ ë„˜ì–´ê°€ê¸°
        if (hasAIResponse) {
            // í˜„ì¬ í™”ë©´ì—ì„œ ìƒˆë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ê³„ì† ì§„í–‰
            // AI ì‘ë‹µì´ ì´ë¯¸ currentQuestionTextì— ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ
            // ì§ˆë¬¸ ì™„ë£Œ ìƒíƒœë§Œ ë¦¬ì…‹í•˜ì—¬ ìƒˆë¡œìš´ ì§ˆë¬¸ì´ í‘œì‹œë˜ë„ë¡ í•¨
            setIsQuestionComplete(false);
            setTranscribedText(null);
            setHasAIResponse(false); // AI ì‘ë‹µ ìƒíƒœ ë¦¬ì…‹
        } else {
            console.log('AI ì‘ë‹µì„ ì•„ì§ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
        }
    };

    const handleEndChat = async () => {
        try {
            // ëŒ€í™” ì„¸ì…˜ ì¢…ë£Œ API í˜¸ì¶œ
            if (conversationId) {
                const endResponse = await conversationApiService.endConversation(conversationId);
                console.log('ëŒ€í™” ì„¸ì…˜ ì¢…ë£Œë¨:', endResponse);
                
                if (endResponse && endResponse.status === 'COMPLETED') {
                    // ì¼ê¸° ìƒì„± ë¡œë”© í™”ë©´ìœ¼ë¡œ ì´ë™
                    navigation.navigate('DiaryLoading');
                    
                    // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ ìƒíƒœ í™•ì¸ ë° ì¼ê¸° ì¡°íšŒ
                    await checkProcessingAndGetDiary(conversationId);
                }
            }
        } catch (error) {
            console.error('ëŒ€í™” ì¢…ë£Œ ì‹¤íŒ¨:', error);
            Alert.alert('ì˜¤ë¥˜', 'ëŒ€í™”ë¥¼ ì¢…ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
        }
    };

    const checkProcessingAndGetDiary = async (conversationId: number) => {
        try {
            // ì²˜ë¦¬ ìƒíƒœ í™•ì¸ (í´ë§)
            const checkStatus = async (): Promise<void> => {
                const statusResponse = await conversationApiService.getProcessingStatus(conversationId);
                
                if (statusResponse && !statusResponse.isProcessing) {
                    // ì²˜ë¦¬ ì™„ë£Œ - ì¼ê¸° ì¡°íšŒ
                    const diaryResponse = await conversationApiService.getDiary(conversationId);
                    
                    if (diaryResponse) {
                        // DiaryResult í™”ë©´ìœ¼ë¡œ ì´ë™
                        navigation.navigate('DiaryResult', {
                            diary: diaryResponse.diary,
                            conversationId: diaryResponse.conversationId,
                            finalEmotion: diaryResponse.emotionSummary.dominantEmotion,
                            userId: "1",
                            musicRecommendations: diaryResponse.musicRecommendations
                        });
                    } else {
                        console.error('ì¼ê¸° ì¡°íšŒ ì‹¤íŒ¨');
                        navigation.navigate('MainTabs' as never);
                    }
                } else {
                    // ì•„ì§ ì²˜ë¦¬ ì¤‘ - 2ì´ˆ í›„ ë‹¤ì‹œ í™•ì¸
                    setTimeout(checkStatus, 2000);
                }
            };
            
            // ì²« ë²ˆì§¸ ìƒíƒœ í™•ì¸ ì‹œì‘
            checkStatus();
        } catch (error) {
            console.error('ì¼ê¸° ìƒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);
            navigation.navigate('MainTabs' as never);
        }
    };

    const handleAnswerRecordingComplete = async (audioUri: string, questionId: string) => {
        console.log(`ë‹µë³€ ë…¹ìŒ ì™„ë£Œ - ì§ˆë¬¸ ID: ${questionId}, ì˜¤ë””ì˜¤ URI: ${audioUri}`);
        setIsRecording(false);
    };

    const handleAIResponse = async (userText: string, audioBase64?: string, conversationMessageId?: number) => {
        console.log('ì‚¬ìš©ì ë°œí™” í…ìŠ¤íŠ¸ ë°›ìŒ:', userText);
        console.log('conversationMessageId ë°›ìŒ:', conversationMessageId);
        try {
            setIsProcessingResponse(true);
            
            // ì‚¬ìš©ì ë°œí™” í…ìŠ¤íŠ¸ ì €ì¥
            setTranscribedText(userText);
            
            // ê°ì • ë¶„ì„ ê²°ê³¼ ì „ì†¡ (ì‹¤ì œ conversationMessageId ì‚¬ìš©)
            if (emotionCaptures.length > 0 && conversationMessageId) {
                const emotionCounts = emotionCaptures.reduce((acc, capture) => {
                    acc[capture.emotion] = (acc[capture.emotion] || 0) + 1;
                    return acc;
                }, {} as Record<string, number>);

                const averageConfidence = emotionCaptures.reduce((sum, capture) => sum + capture.confidence, 0) / emotionCaptures.length;
                const finalEmotion = Object.keys(emotionCounts).reduce((a, b) => emotionCounts[a] > emotionCounts[b] ? a : b);

                console.log('ê°ì • ë¶„ì„ ê²°ê³¼ ì „ì†¡ - conversationMessageId:', conversationMessageId);
                
                try {
                    await sendFacialEmotionAnalysis({
                        conversationMessageId: conversationMessageId,
                        finalEmotion,
                        totalCaptures: emotionCaptures.length,
                        emotionCounts,
                        averageConfidence,
                        captureDetails: emotionCaptures
                    });
                    console.log('í‘œì • ê°ì • ë¶„ì„ ì €ì¥ ì™„ë£Œ');
                    
                    // í‘œì • ê°ì • ë¶„ì„ ì €ì¥ ì™„ë£Œ í›„ KoBERT í”Œë¡œìš° ì‹¤í–‰
                    await executeKoBERTFlow(conversationMessageId, userText);
                } catch (error) {
                    console.error('âŒ í‘œì • ê°ì • ë¶„ì„ ì €ì¥ ì‹¤íŒ¨:', error);
                    // í‘œì • ê°ì • ë¶„ì„ ì €ì¥ ì‹¤íŒ¨ ì‹œì—ë„ KoBERT í”Œë¡œìš° ì‹¤í–‰
                    await executeKoBERTFlow(conversationMessageId, userText);
                }
            } else {
                // í‘œì • ê°ì • ë¶„ì„ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ KoBERT í”Œë¡œìš° ì‹¤í–‰
                if (conversationMessageId) {
                    await executeKoBERTFlow(conversationMessageId, userText);
                }
            }

            // GPT ë‹µë³€ ìƒì„± (ì‹¤ì œ conversationMessageId ì‚¬ìš©)
            if (conversationMessageId) {
                const gptResponse = await gptApiService.generateResponse({
                    conversationMessageId: conversationMessageId
                });

                if (gptResponse) {
                    // AI ì‘ë‹µìœ¼ë¡œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                    setCurrentQuestionText(gptResponse.aiResponse);
                    setHasAIResponse(true);
                    setIsQuestionComplete(false);

                    // AI ì‘ë‹µ TTS ì¬ìƒ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                    try {
                        console.log('ğŸµ TTS ìš”ì²­ ì‹œì‘:', gptResponse.aiResponse);
                        const ttsResponse = await ttsApiService.synthesize({
                            text: gptResponse.aiResponse,
                            voice: 'ko-KR-Wavenet-A',
                            speed: 1.0,
                            pitch: 0.0,
                            volume: 0.0,
                            format: 'MP3'
                        });

                        if (ttsResponse.status === 'success' && ttsResponse.audioData) {
                            await ttsService.playAudio(ttsResponse.audioData, 'mp3');
                            console.log('ğŸµ AI ì‘ë‹µ TTS ì¬ìƒ ì™„ë£Œ - ë§ˆì´í¬ ë‹¤ì‹œ í™œì„±í™”');
                        } else {
                            console.error('âŒ TTS ì‘ë‹µì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ:', ttsResponse);
                        }
                    } catch (error) {
                        console.error('AI ì‘ë‹µ TTS ì¬ìƒ ì‹¤íŒ¨:', error);
                    }
                }
                
                // AI ì‘ë‹µì„ ë°›ì•˜ìŒì„ í‘œì‹œ
                setHasAIResponse(true);
                
                // TTS ì¬ìƒ ì™„ë£Œ í›„ ë§ˆì´í¬ ìƒíƒœ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆë„ë¡)
                setTranscribedText('');
                setEmotionCaptures([]);
                setIsQuestionComplete(true); // ë§ˆì´í¬ ë²„íŠ¼ì„ ë‹¤ì‹œ í™œì„±í™”
            }
            
        } catch (error) {
            console.error('AI ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);
        } finally {
            setIsProcessingResponse(false);
        }
    };

    // KoBERT í”Œë¡œìš° ì‹¤í–‰ í•¨ìˆ˜
    const executeKoBERTFlow = async (conversationMessageId: number, userText: string) => {
        try {
            console.log('ğŸ”„ KoBERT í”Œë¡œìš° ì‹œì‘ - conversationMessageId:', conversationMessageId);

            // 1. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
            const contextResponse = await conversationContextApiService.getContext(conversationMessageId);
            console.log('ğŸ“ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì™„ë£Œ:', contextResponse);

            // 2. KoBERT ê°ì • ë¶„ì„
            const kobertResponse = await kobertApiService.predictEmotion({
                prev_user: contextResponse.prevUser || "",
                prev_sys: contextResponse.prevSys || "",
                curr_user: contextResponse.currUser || ""
            });
            console.log('KoBERT ê°ì • ë¶„ì„ ì™„ë£Œ:', kobertResponse);

            // all_probabilitiesì—ì„œ ê°€ì¥ ë†’ì€ ê°’ ì°¾ê¸°
            const allProbabilities = kobertResponse.all_probabilities;
            const maxEmotion = Object.keys(allProbabilities).reduce((a, b) => 
                allProbabilities[a as keyof typeof allProbabilities] > allProbabilities[b as keyof typeof allProbabilities] ? a : b
            );
            const maxConfidence = allProbabilities[maxEmotion as keyof typeof allProbabilities];

            // 3. ìŒì„± ê°ì • ë¶„ì„ ì €ì¥
            const speechEmotionData = {
                text: userText,
                analysisResult: {
                    emotion: maxEmotion,
                    confidence: maxConfidence,
                    details: kobertResponse
                }
            };
            
            const speechEmotionResponse = await speechEmotionApiService.saveSpeechEmotion({
                conversationMessageId: conversationMessageId,
                emotion: maxEmotion,
                confidence: maxConfidence,
                speechEmotionData: JSON.stringify(speechEmotionData)
            });
            console.log('ìŒì„± ê°ì • ë¶„ì„ ì €ì¥ ì™„ë£Œ:', speechEmotionResponse);

            // 4. í†µí•© ê°ì • ë¶„ì„ ì‹¤í–‰
            try {
                console.log('í†µí•© ê°ì • ë¶„ì„ ì‹œì‘ - conversationMessageId:', conversationMessageId);
                console.log('í‘œì • ê°ì • ìº¡ì²˜ ìˆ˜:', emotionCaptures.length);
                const combinedEmotionResponse = await combinedEmotionApiService.combineEmotions({
                    conversationMessageId: conversationMessageId
                });
                console.log('í†µí•© ê°ì • ë¶„ì„ ì™„ë£Œ:', combinedEmotionResponse);
            } catch (error) {
                console.error('í†µí•© ê°ì • ë¶„ì„ ì‹¤íŒ¨:', error);
                console.log('í‘œì • ê°ì •ê³¼ ë§ ê°ì •ì´ ëª¨ë‘ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.');
            }

        } catch (error) {
            console.error('KoBERT í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜:', error);
        }
    };

    const handleAnswerRecordingStart = async (questionId: string) => {
        console.log(`ë‹µë³€ ë…¹ìŒ ì‹œì‘ - ì§ˆë¬¸ ID: ${questionId}`);
        setIsRecording(true);
    };


    return (
        <SafeAreaView className="flex-1 bg-white">
            {/* ìˆ¨ê²¨ì§„ ì¹´ë©”ë¼ (ì‹¤ì‹œê°„ ì´ë¯¸ì§€ ì „ì†¡) */}
            <HiddenCamera 
                isRecording={isRecording}
                onFaceDetected={(imageData) => {
                    // HiddenCameraì—ì„œ ì´ë¯¸ ê°ì • ë¶„ì„ì´ ì™„ë£Œëœ ê²°ê³¼ë¥¼ ë°›ìŒ
                    console.log('ğŸ“¸ ì´ë¯¸ì§€ ê°ì • ë¶„ì„ ê²°ê³¼ ìˆ˜ì‹ :', imageData);
                    
                    if (imageData.emotionResult && imageData.emotionResult.success && isRecording) {
                        const newCapture = {
                            timestamp: imageData.timestamp,
                            emotion: imageData.emotionResult.emotion,
                            confidence: imageData.emotionResult.confidence
                        };
                        
                        setEmotionCaptures(prev => [...prev, newCapture]);
                        console.log('ê°ì • ìº¡ì²˜ ì¶”ê°€ë¨:', newCapture);
                    }
                }}
                onRecordingStart={() => {
                    console.log('HiddenCamera: ë…¹ìŒ ì‹œì‘ë¨');
                    // ê°ì • ìº¡ì²˜ ì´ˆê¸°í™”ëŠ” ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ ì‹œì—ë§Œ ìˆ˜í–‰
                    // setEmotionCaptures([]); // ì£¼ì„ ì²˜ë¦¬
                }}
                onRecordingStop={() => {
                    console.log('HiddenCamera: ë…¹ìŒ ì¢…ë£Œë¨');
                }}
            />

            {/* ë©”ì¸ ì»¨í…ì¸  */}
            <View className="flex-1 p-6">
                {/* AI ì§ˆë¬¸ ì„¹ì…˜ */}
                <AIQuestionSection 
                    questionText={currentQuestionText}
                    onQuestionComplete={handleQuestionComplete}
                />

                {/* ì²˜ë¦¬ ì¤‘ ìƒíƒœ í‘œì‹œ */}
                {isProcessingResponse && (
                    <View className="items-center mb-8">
                        <View className="bg-blue-100 px-6 py-4 rounded-full">
                            <Text className="text-blue-600 font-medium text-center">
                                AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...
                            </Text>
                        </View>
                    </View>
                )}

                {/* ì‚¬ìš©ì ë‹µë³€ ì„¹ì…˜ */}
                {isQuestionComplete && !isProcessingResponse && (
                    <UserAnswerSection
                        questionId={questionId || 'default-question-id'}
                        microphoneSessionId={microphoneSessionId || null}
                        cameraSessionId={cameraSessionId || null}
                        conversationId={conversationId?.toString() || null}
                        userId={userId || null}
                        onRecordingComplete={handleAnswerRecordingComplete}
                        onRecordingStart={handleAnswerRecordingStart}
                        onAIResponse={handleAIResponse}
                        onNext={handleNext}
                        onEndChat={handleEndChat}
                        transcribedText={transcribedText}
                        isRecording={isRecording}
                        isQuestionComplete={isQuestionComplete}
                        hasAIResponse={hasAIResponse}
                    />
                )}
            </View>
        </SafeAreaView>
    );
}
